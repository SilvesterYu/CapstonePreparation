# Transformer Notes

### Attention
In NLP, attention is basically measuring how each word in the input sentence is associated with each word on the output translated sentence. Similarly, there is also what we call self-attention that could be seen as a measurement of a specific wordâ€™s effect on all other words of the same sentence. 
