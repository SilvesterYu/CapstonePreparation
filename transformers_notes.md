# Transformer Notes

### Attention
In NLP, attention is basically measuring how each word in the input sentence is associated with each word on the output translated sentence.
