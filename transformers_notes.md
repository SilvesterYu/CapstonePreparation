# Transformer Notes

### Attention
attention is basically measuring how each word in the input sentence is associated with each word on the output translated sentence.
